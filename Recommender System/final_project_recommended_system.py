# -*- coding: utf-8 -*-
"""Final Project Recommended System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13W-cSceRZS-fMzDhua1cGlBoDwFv2hgq

# Sistem Rekomendasi Musik

Nama : Renol N

Asal : Majalengka

Kenapa menggunakan Kaggle Notebook ? Google Colab Limit Memory Check Specification of Kaggle Memory
![](https://miro.medium.com/max/1400/1*WeJ3Gtr9D-k31lCoj_5rvw@2x.jpeg)
"""

# Extract Datasets nya
!7z e "../input/kkbox-music-recommendation-challenge/train.csv.7z"
!7z e "../input/kkbox-music-recommendation-challenge/members.csv.7z"
!7z e "../input/kkbox-music-recommendation-challenge/songs.csv.7z"

"""# Data Understanding

Memberikan informasi seperti jumlah data, kondisi data, dan informasi mengenai data yang digunakan.

Menuliskan tautan sumber data (link download). 
https://www.kaggle.com/competitions/kkbox-music-recommendation-challenge/data

Menguraikan seluruh variabel atau fitur pada data.

Melakukan beberapa tahapan yang diperlukan mengenai data, contohnya teknik visualisasi data beserta insight atau exploratory data analysis.

## Answers of Data Understanding

### 1
#### train.csv : (7377418 data, 6 columns)
    
    msno: user id
    
    song_id: song id
    
    source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search.
    
    source_screen_name: name of the layout a user sees.
    
    source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc.
    
    target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise .
#### test.csv : (data, columns)
    
    id: row id (will be used for submission)
    
    msno: user id
    
    song_id: song id
    
    source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search.
    
    source_screen_name: name of the layout a user sees.
    
    source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc.
    
#### sample_submission.csv (We not use this file)

#### songs.csv
The songs. Note that data is in unicode.

    song_id
    
    song_length: in ms
    
    genre_ids: genre category. Some songs have multiple genres and they are separated by |
    
    artist_name
    
    composer
    
    lyricist
    
    language

#### members.csv
user information.

    msno
    
    city
    
    bd: age. Note: this column has outlier values, please use your judgement.
    
    gender
    
    registered_via: registration method
    
    registration_init_time: format %Y%m%d
    
    expiration_date: format %Y%m%d
    
    song_extra_info.csv
    
    song_id
    
    song name - the name of the song.
    
    isrc - International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times.
    
### 2

https://www.kaggle.com/competitions/kkbox-music-recommendation-challenge/data

### 3

Check Output Execution Code

### 4

Check Output Execution Code
"""

import pandas as pd
train_data = pd.read_csv('./train.csv')
songs = pd.read_csv('./songs.csv')
member = pd.read_csv('./members.csv')

train_data.head()

songs.head()

member.head()

train_data.info()

songs.info()

member.info()

print("Shape Train Data : ", train_data.shape)
print("Shape Songs Data : ", songs.shape)
print("Shape Songs Info Data : ", member.shape)

import numpy as np
df_all = np.concatenate([
    train_data.song_id.unique(),
    songs.song_id.unique(),
    member.msno.unique()
])

df_all = np.sort(np.unique(df_all))

print("Jumlah seluruh datasets: ", len(df_all))

# Merge

df = pd.merge(train_data, songs, on='song_id', how='left')

df = pd.merge(df, member, on='msno', how='left')
df.head()

df.info()

dtypes = pd.DataFrame(df.dtypes,columns=["Data Type"])

dtypes["Unique Values"]=df.nunique().sort_values(ascending=True)

dtypes["Null Values"]=df.isnull().sum()

dtypes["% null Values"]=df.isnull().sum()/len(df)

dtypes.sort_values(by="Null Values" , ascending=False).style.background_gradient(axis=0)

"""# Data Preparation

Menerapkan dan menyebutkan teknik data preparation yang dilakukan.

Teknik yang digunakan pada notebook dan laporan harus berurutan.

Menjelaskan proses data preparation yang dilakukan.

Menjelaskan alasan mengapa diperlukan tahapan data preparation tersebut.

Handling null Value, ini perlu sebab data yang digunakan jauh dari kata bersih / clean
"""

df.isnull().sum()

df = df.dropna()

df.isnull().sum()

df.info()

df.shape

"""Filter Column yang akan kita gunakan, ini perlu karena tidak semua kolom akan kita gunakan, hanya kolom yang dirasa akan dibutuhkan oleh requirement tim pengembang"""

used_columns = ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'target', 'artist_name', 'genre_ids', 'composer', 'gender']
new_df = df[used_columns]

new_df.head()

new_df.isnull().sum()

"""Banyak value yang unbalance, misal di bagian genre, ada 1 genre berisi 50.000 value sedangkan ada genre yang hanya berisi 1 value... maka dari itu akan kita proses genre yang value nya kurang dari 1000 akan kita drop atau hapus"""

for column in new_df.columns:
 print("\n" + column)
 print(new_df[column].value_counts())

"""Saya sadar disini composer dan msno tidak terlalu perlu karena terlalu banyak noise yang akan dihasilkan, dan msno ini lebih kayak ke user ID (Ini lebih cocok jika personalized recommendation)"""

new_df = new_df.drop(['composer', 'msno'], axis=1)
new_df

"""### Problem


Data terlalu banyak jadi cosine smiliarity sangat memakan waktu lama jadi akan kita buat conditions ketika artist_name kurang dari 1000 akan kita drop, lalu akan kita drop juga song_id yang sama agar mengurangi data
"""

df_new = new_df.drop_duplicates(subset='song_id')
df_new.shape

counts = df_new['artist_name'].value_counts()
df = df_new[~df_new['artist_name'].isin(counts[counts < 100].index)]

counts = df['genre_ids'].value_counts()
df = df[~df['genre_ids'].isin(counts[counts < 100].index)]

"""## EDA"""

df.shape

df.info()

import matplotlib.pyplot as plt
import seaborn as sns

cat_features = ['source_system_tab', 'source_screen_name', 'source_type', 'target', 'gender']

for col in range(len(cat_features)):
    plt.figure()
    plt.xticks(rotation=90)
    plt.title(f'Count Plot Column {cat_features[col]}')
    sns.countplot(df[cat_features[col]])

"""Disimpulkan fitur yang, akan diambil sudah tepat

label encoder untuk mengubah data kategorikal menjadi data number
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['song_id_le'] = le.fit_transform(df['song_id'])
df['source_system_tab'] = le.fit_transform(df['source_system_tab'])
df['source_screen_name'] = le.fit_transform(df['source_screen_name'])
df['source_type'] = le.fit_transform(df['source_type'])

"""Cek Korelasi"""

# Сorrelation matrix
plt.figure(figsize=[15,10])
sns.heatmap(df.corr(), annot=True)
plt.show()

df.reset_index(drop=True)

"""# Modeling and Result

## Cosine Smiliarity / Content Based Similarity

Membuat dan menjelaskan sistem rekomendasi untuk menyelesaikan permasalahan

Menyajikan top-N recommendation sebagai output.

Menyajikan dua solusi rekomendasi dengan algoritma yang berbeda.

Menjelaskan kelebihan dan kekurangan pada pendekatan yang dipilih.

Cosine similarity mengukur kesamaan antara dua vektor dan menentukan apakah kedua vektor tersebut menunjuk ke arah yang sama. Ia menghitung sudut cosinus antara dua vektor. Semakin kecil sudut cosinus, semakin besar nilai cosine similarity. 

Metrik ini sering digunakan untuk mengukur kesamaan dokumen dalam analisis teks. Sebagai contoh, dalam studi kasus ini, cosine similarity digunakan untuk mengukur kesamaan nama restoran dan nama masakan.

Maka dari itu saya gunakan Similarity untuk mencari kesamaan genre dan gender untuk rekomendasi
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

cv = CountVectorizer()

cosim_df = cv.fit_transform(df['artist_name'])
cosine_sim = cosine_similarity(cosim_df)

"""lupa belum di label encoder, soalnya bukan inputan"""

df['artist_name'] = le.fit_transform(df['artist_name'])

def recommend_songs_to_you(df_resm, genre, gender):
  try:
    genre_gender_data = df_resm[(df_resm['gender'] == gender) & (df_resm['genre_ids'] == genre)]
    similar_songs = df_resm.copy()

    sound_prop = similar_songs.loc[:, ['song_id_le', 'source_system_tab', 'source_screen_name', 'source_type', 'target', 'artist_name']]

    similar_songs['Similarity with songs'] = cosine_similarity(sound_prop, sound_prop.to_numpy()[genre_gender_data.index[0], None]).squeeze()

    similar_songs.rename(columns={'genre_ids': f'Similar with genre ids {genre}'}, inplace=True)
    similar_songs['artist_name'] = le.inverse_transform(similar_songs['artist_name'])
    similar_songs = similar_songs.sort_values(by='Similarity with songs', ascending=False)
    similar_songs = similar_songs[['song_id', f'Similar with genre ids {genre}', 'artist_name', 'gender']]

    similar_songs.reset_index(drop=True, inplace=True)

    return similar_songs.iloc[1:7]
  except Exception as e:
    print('error: ', e)

"""top recommended songs"""

recommend_songs_to_you(df, '465', 'male')

"""# Colaborative Filtering

Untuk Colaborative filtering yang akan saya gunakan adalah userid, songs name dan target
"""

!7z e "../input/kkbox-music-recommendation-challenge/song_extra_info.csv.7z"

"""baca datasets, karena baru kali ini kita pakai datasets song_extra_info untuk mengambil judul lagu"""

song_info = pd.read_csv('./song_extra_info.csv')
song_info.head()

"""merge semua data yang diperlukan"""

df = pd.merge(train_data, songs, on='song_id', how='left')

df = pd.merge(df, song_info, on='song_id', how='left')
df.head()

"""handling null value"""

df.isnull().sum()

"""ambil 100.000 sample"""

df = df.sample(n=100000)

df = df.reset_index(drop=True)

"""pilih column yang diperlukan"""

df = df[['target', 'msno', 'name']]

df

"""Encode fitur msno / user id dan name atau nama lagunya"""

user_ids = df['msno'].unique().tolist()
song_ids = df['name'].unique().tolist()

"""Menyandikan (encode) fitur ‘msno’ dan ‘name’ ke dalam indeks integer. 

Memetakan ‘msno’ dan ‘name’ ke dataframe yang berkaitan.

Mengecek beberapa hal dalam data seperti jumlah user, jumlah lagu, kemudian mengubah target menjadi float.
"""

# Melakukan encoding msno
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
 
# Melakukan proses encoding angka ke ke msno
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Melakukan proses encoding song
song_to_song_encoded = {x: i for i, x in enumerate(song_ids)}
 
# Melakukan proses encoding angka ke song
song_encoded_to_song = {i: x for i, x in enumerate(song_ids)}

# Mapping user ke dataframe user
df['msno'] = df['msno'].map(user_to_user_encoded)
 
# Mapping song ke dataframe song
df['name'] = df['name'].map(song_to_song_encoded)

# Mengubah target menjadi nilai float
df['target'] = df['target'].values.astype(np.float32)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah msno/user
num_song = len(song_encoded_to_song)
print(num_song)

# Mengacak Datasets
df = df.sample(frac=1, random_state=42)
df

"""Split dengan rasio 8:2

80.000 data train

20.000 data val
"""

# Membuat variabel x untuk mencocokkan data user dan song menjadi satu value
x = df[['msno', 'name']].values
 
# Membuat variabel y untuk membuat target dari hasil 
y = df['target']
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

"""Membuat Recommender Net berikut example nya

https://keras.io/examples/structured_data/collaborative_filtering_movielens/
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_song, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_song
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_song,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_song, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    song_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    song_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_song = tf.tensordot(user_vector, song_vector, 2) 
 
    x = dot_user_song + user_bias + song_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""Compile model dengan loss RMSE"""

model = RecommenderNet(num_users, num_song, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""# Evaluation

Menyebutkan metrik evaluasi yang digunakan. RMSE

Menjelaskan hasil proyek berdasarkan metrik evaluasi.

Menjelaskan metrik evaluasi yang digunakan untuk mengukur kinerja model (formula dan cara metrik tersebut bekerja).

rmse = $\sqrt{\sum\nolimits_{n=1}^n \left((y_i - ŷ_p) ^ 2 \over n \right) }$  


Visualisasi Metrix untuk Melihat error dalam bentuk grafik
"""

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

!7z e "../input/kkbox-music-recommendation-challenge/test.csv.7z"

df_test = pd.read_csv('./test.csv')
df_test.head()

df_test.shape

user_id = df_test.msno.sample(1).iloc[0]
music_listen_by_user = df_test[df_test.msno == user_id]

music_listen_by_user

"""Disini saya column song_id sudah di drop jadi load ulang"""

df = pd.merge(train_data, songs, on='song_id', how='left')

df = pd.merge(df, song_info, on='song_id', how='left')
df.head()
df.isnull().sum()
df = df.sample(n=100000)
df = df.reset_index(drop=True)
df = df[['target', 'msno', 'name', 'song_id']]

music_df = df

music_df

music_not_listen = music_df[~music_df['song_id'].isin(music_listen_by_user.song_id.values)]['name'] 
music_not_listen = list(
    set(music_not_listen)
    .intersection(set(song_to_song_encoded.keys()))
)
len(music_not_listen)

music_not_listen = [[song_to_song_encoded.get(x)] for x in music_not_listen]

user_encoder = user_to_user_encoded.get(user_id)
user_song_array = np.hstack(
    ([[user_encoder]] * len(music_not_listen), music_not_listen)
)

len(user_song_array) - len([x for x in user_song_array if x is not None])

targets = model.predict(user_song_array).flatten()
targets

top_ratings_indices = targets.argsort()[-10:][::-1]
recommended_song_ids = [
    song_encoded_to_song.get(music_not_listen[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('song with high ratings from user')
print('----' * 8)
 
top_song_user = (
    music_listen_by_user.sort_values(
        by = 'song_id',
        ascending=False
    )
    .head(5)
    .song_id.values
)
 
song_df_rows = music_df[music_df['song_id'].isin(top_song_user)]
for row in song_df_rows.head(5).itertuples():
    print(row.name)
 
print('----' * 8)
print('Top 10 song recommendation')
print('----' * 8)
 
recommended_song = music_df[music_df['name'].isin(recommended_song_ids)]
for row in recommended_song.head(10).itertuples():
    print(row.name)

